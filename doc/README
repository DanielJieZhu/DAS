This is an proposal for DAS QL implementation. I will only outline a few use cases, discuss workflow and possible APIs.

I. DAS Requirements:
=================
1. provide simple, intuitive QL for end-users
2. support keys.attributes for different data-services
3. support boolean expressions
4. be data-service independent
5. use data-service existing APIs

II. DAS implementation:
===================
it will require the following information from data-services participating in DAS.
1. a list of keys (entities) which data service support, e.g.
   DBS: dataset, run, site, file, ....
   SiteDB: admin, cmsname, ...
   Phedex: rate, storage, pfn, ...

Services may have overlapping keys, e.g. site is defined in DBS and SiteDB, but DBS has higher priority. So we may end-up of assigning priorities. See example below.

2. A mapping between keys and data-service APIs, e.g.
   For DBS {'dataset': executeQuery('find dataset'), ...}, in a case of DBS most of the keys will be retrieved via single API based on DBS-QL, but it's not a requirement.
   For SiteDB {'admin': sitedb_api_to_retreive_admin, ...}
   For Phedex {'rate': phedex_api_to_retrieve_rate, ...}

III. DAS APIs:
===========
- plot(service, query), execute find(service, query) and invoke plotting function to draw and represent plot
- call(service, query), execute query at given service, by using service API
- mapping(service), retrieve mapping (II.2) for given service, return type dict[key]=service_api
- key(service1, service2), return a relation key between two services
- create_view(set), create couchdb view for provided set

IV. DAS internals:
===============
- holds the following data
  keys = {'service':[listOfKeys]}, e.g. {'dbs':['dataset', 'run', 'file', ...]}
  inter_service_map = { (service1, service2):key }, e.g. { ('dbs','sitedb'):'site', ('dbs','phedex'):'block', ('dbs','runsummary'):'run'}
  service_apis = {'service':dictOfserviceAPis}, e.g. {'phedex': {'rate': api, 'storage': api}}

V. DAS example QL
================

User type the following query
find dataset, run, bfield where site=T2_US and admin=VK and storage=castor

obviously we need to call DBS, SiteDB, Phedex, RunSummary. 

Step 0: define query = find dataset, run, bfield where site=T2_US and admin=VK and storage=castor
Step 1: extract selection keywords, selList = [dataset, run, bfield]
Step 2: extract where clause keywords whereList = [site, admin, storage]
Step 3: identify data-service in question, walk through selList + whereList and using DAS internals define that we need to call, 
in our case it is DBS, RunSummary, SiteDB, Phedex
Step 4: create unique set of keys to retrieve from all services by using service relation map
(DBS, RunSummary): run
(DBS, SiteDB): site
(DBS, Phedex): block
(RunSummary, SiteDB): none
(RunSummary, Phedex): none
(SiteDB, Phedex): none

so unique set of keys would be

sel = [dataset, run, bfield, site, block]

Step 5: create service-independent queries, by that I mean take where clause apart and construct new set of queries, the query MUST contain a single where clause which will define which service to call. For this example we will end-up with the following

Q1 = find sel where site=T2_US (will call DBS site DBS contains site as a keyword and it has higher priority wrt SiteDB)
Q2 = find sel where admin=VK (will call SiteDB, since it contains admin key)
Q3 = find sel where storage=castor (will call Phedex since it contains storage key)
Q4 = find sel (call RunSummary, even though we don't have any where clause we need to call it since our unique set of keys contain a key from this data service)

Step 5: construct workflow

workflow = ( (call DBS with Q1), (call SiteDB with Q2), (call Phedex with Q3), (call RunSummary with Q4) )

Step 6: using DAS internal mapping convert queries into data-serive API calls

workflow = ( das_call(dbs, Q1), das_call(sitedb, Q2), das_call(phedex, Q3), das_call(runsummary, Q4) )

Step 7: executing all apis in parallel (they are independent !!!). As you see from step 6 I invoke das_call, his signature see III., is
call(service, query)

now this function should keep a unique set of keys to return
sel = [dataset, run, bfield, site, block]

but ask particular data-service to return appropriate key. So in case of DBS it will ask for dataset, run, site, block. In case of SiteDB it will ask site. In case of RunSummary it will ask for run, bfield, in case of Phedex it will ask for block

So the output from data das_call should be in a form of unique set of keys, which is in our case (dataset, run, bfield, site, block), here are examples (I will use 'x' as some value and ? as null:

DBS returns    [ (x, x, ?, x, x), ... ] 
SiteDB returns [ (?, ?, ?, x, ?), ... ]
RunSum returns [ (?, x, x, ?, ?), ... ]
Phedex returns [ (?, ?, ?, ?, x), ... ]

Step 8: DAS collects results and invoke create_view with returning sets. So I would say invoke couchDB and insert all results.

If couchDB is capable to construct cartesian product from those sets then we're done, otherwise we need to do it manually.

Comments:
=========
One limitation I see is that if we will allow brackets in where clauses I would restrict their usage to one service. Only keys from one service can be used in brackets. Example

find ... where (run=1 or run=2) and storage=castor OK

find ... where (run=1 and storage=castor) or (run=2 and storage=tape) NOT OK

this is because I want to grab everything in bracket and consider it as one where clause for data-service. Otherwise parser can be VERY complicated.

VI. Efficiency of the method.
Step 0-6 are trivial and I don't expect any timing issues with that.
Step 7 is nice in a way that all queries are sent independently, so I would say that DBS execution will be dominant. But I do still see that all this queries can be done in order of seconds if pagination will be used. The hardest part is Step 8. If couchDB is capable of multiplex results from returning sets (basically cartesian product should be done), then we are golden. If not we can use AJAX and do it in background. Basically it means to do the following:

(DBS result) X (SiteDB result) X (RunSum result) X (Phedex result).

Based on amount of information we have we can do pagination from DBS, while retrieve everything from others and make this product.

The rest (caching, presentation, views, etc.) can be done by using couchDB.


