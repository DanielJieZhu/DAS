#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[margin=1.5in]{geometry}
\usepackage[cm]{fullpage}
% enable Hyperlinks
\usepackage{hyperref}
% fancy verbatim
\usepackage{fancyvrb}
\usepackage{appendix}


%This command will change the default Bibliography to References
\AtBeginDocument{
      % \renewcommand{\bibname}{References}
        %\renewcommand{\nomname}{List of Symbols}
      \addtolength{\textheight}{10mm}
}


\usepackage{minitoc}

% Redefining automatic label of Figure, Table and Eq.
\usepackage{prettyref}
\newrefformat{tab}{Table\,\ref{#1}}
\newrefformat{fig}{Figure\,\ref{#1}}
\newrefformat{eq}{Eq.\,\textup{(\ref{#1})}}

\bibliographystyle{alpha} 


%\usepackage{fullpage}
\end_preamble
\options a4paper,11pt
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\notefontcolor #7d7d7d
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Keyword Search over Data Services: January Report
\end_layout

\begin_layout Author
Vidmantas Zemleris
\end_layout

\begin_layout Section
From Keywords to Structured Queries
\end_layout

\begin_layout Standard
Mainly there are two methods to translating Keywords to Structured Queries:
 a heuristic rules ranking based exhaustively evaluating possible alternatives,
 and machine learning approach.
 The full sentence Question Answering (QA) approach is also possible, however
 the keyword-based methods (possibly with ideas from QA) shall work better
 knowing the constraints on the short project duration.
\end_layout

\begin_layout Subsubsection*

\color red
\begin_inset Note Note
status open

\begin_layout Subsubsection*
Definitions
\end_layout

\begin_layout Description
Schema: by schema we refer to the integration schema (virtual schema based
 of entities exposed by the services)
\end_layout

\begin_layout Description
Schema 
\series bold
terms: 
\series default
- names of entities or their attributes
\color red
, and their values (domain of possible values); including inputs and outputs
 of data services
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Exhaustive search & Heuristics (based on Keymantic
\begin_inset CommandInset citation
LatexCommand cite
key "keymantic10"

\end_inset

)
\begin_inset CommandInset label
LatexCommand label
name "sub:Heuristics-method-(based"

\end_inset


\end_layout

\begin_layout Standard
Currently the prototype is based on a simple recursive exhaustive search.
 It starts with some matching techniques
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
including a number of metrics for lexical and semantic word similarity and
 regular expressions defining allowed inputs for generating entry points
 (this could result in false positives with the likelihood slightly lower
 than of the true positives);
\end_layout

\begin_layout Plain Layout
Limitation: Semantic Word Similarity is based on NLTK & Wordnet which currently
 works only on the same parts of speech.
 E.g.
 'located at XX' can not be easily matched to 'site' (that means 'location'
 in our domain) automatically.
\end_layout

\end_inset

 to identify the schema terms or their values to which each keyword may
 correspond (entry points) estimating each 
\begin_inset Quotes eld
\end_inset

likelihood
\begin_inset Quotes erd
\end_inset

.
 Then these choices are combined with a number of heuristics obtaining a
 score of possible result, and these are ranked.
 It's flexible for defining the features deciding the rank, but currently
\color magenta
 it is hard to add feedback (especially the implicit one) 
\color inherit
and heuristics' parameters are hand-picked.
\end_layout

\begin_layout Paragraph*

\series bold
Heuristics: Calculating Entry points
\end_layout

\begin_layout Standard
As an example, for query 
\begin_inset Quotes eld
\end_inset

configuration of Zmmg-13Jul2012-v1
\begin_inset Quotes erd
\end_inset

 (the numbers are estimated likelihoods): 'configuration' is matching schema
 terms: (1.0, u'config.name')
\color magenta
, (0.66, u'lumi'), (0.57, u'dataset.name'), (0.54, u'status.name'), (0.51, u'monitor')
;
\color inherit
 
\begin_inset Marginal
status open

\begin_layout Plain Layout

\color magenta
false positive matches colored in Magenta
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While 'Zmmg-13Jul2012-v1' matches as a values of these schema terms: 
\end_layout

\begin_layout Standard
(0.7, 'dataset.name=*Zmmg-13Jul2012-v1*', 
\color magenta
(0.5, u'reco_status'), (0.5, u'tier.name')
\end_layout

\begin_layout Paragraph*

\series bold
Heuristics: Combining entry points
\end_layout

\begin_layout Standard
In addition to combining 
\begin_inset Quotes eld
\end_inset

likelihood
\begin_inset Quotes erd
\end_inset

 scores of entry points, a number of heuristics is applied to boost the
 final scores:
\end_layout

\begin_layout Itemize
promoting such configurations where nearby keywords refer to related schema
 terms (e.g.
 entity name and it's value)
\end_layout

\begin_layout Itemize
promoting such configurations that match most of the keywords; 
\end_layout

\begin_layout Itemize
if requested entity and a filter condition is the same (small increase)
\end_layout

\begin_layout Itemize
data service constraints must be satisfied.

\color magenta
 alternative: identifying interpretations of high rank, that do not satisfy
 some constraint (a mandatory filter condition is missing), and informing
 user
\end_layout

\begin_layout Paragraph*
Performance
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "keymantic10"

\end_inset

 employed an performance optimization based on 
\begin_inset Quotes eld
\end_inset

Munkres/Hungarian
\begin_inset Quotes erd
\end_inset

 algorithm, it makes the search a bit more restrictive
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 because they only consider true/false matchings for some terms, whereas
 in our case many of the matchings (of entry points) are susceptible to
 false positives (e.g.
 regular-expression based)
\end_layout

\end_inset

.
 At the moment, we don't combine services
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
that is not directly supported by data integration system
\end_layout

\end_inset

, so the performance is OK even without this.
 Another possible optimization could be Dynamic Programming based on the
 assumption that interdependences exist only over k-nearby keywords reducing
 the search space (similar to the assumption on HMM, but allowing to any
 scoring/ranking heuristics).
\end_layout

\begin_layout Subsection
Machine learning:
\begin_inset space ~
\end_inset

HMM method (KEYRY
\begin_inset CommandInset citation
LatexCommand cite
key "bergamaschi2011hidden"

\end_inset

)
\end_layout

\begin_layout Standard

\lang british
Bergamaschi et al.
 approached the same problem through Hidden Markov Model (HMM
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "HMM"
description "Hidden Markov Model"

\end_inset

) that allow taking into account the query logs or user's feedback.
 
\series bold
Keywords that are near to each other are expected to represent interrelated
 concepts
\series default
, and HMM can efficiently model such kind of interdependences 
\begin_inset CommandInset citation
LatexCommand citep
key "semantics_without_access"

\end_inset

.
\end_layout

\begin_layout Standard
An HMM Models the keyword translation as probabilistic process (similarly
 as HMM-based speech recognition or part-of-speech-tagging) under the assumption
s that:
\end_layout

\begin_layout Enumerate
the label (HMM state) of each keyword (HMM observable) depends only the
 keyword itself and on a fixed number of labels before/after it -- this
 is mostly true for keyword queries
\end_layout

\begin_layout Enumerate
the probability of transitioning from label 
\emph on
X
\emph default
 to the next label 
\emph on
Y
\emph default
 depends only on last label(s) and the next label.
\end_layout

\begin_layout Standard
These probabilities form the basis of HMM model.
 They could be estimated using some of the heuristics as in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Heuristics-method-(based"

\end_inset

 allowing to use this method even then there is no enough of learning data
 (More details on implementation are on the report draft).
\end_layout

\begin_layout Description
Advantages 
\series bold
of HMM:
\series default
 it can model keyword labeling quite intuitively and allows incorporating
 users feedback easily (at least positive feedback on what of proposed results
 were good).
 
\emph on
It could work even then query log is not available yet (without training).
\end_layout

\begin_layout Description

\series bold
Disadvantages
\series default
: requires modeling each possible word resulting in large model 
\color magenta
(at least it seems so)
\color inherit
; training could work badly with low number of examples (however it is not
 mandatory).
 The implementation aspects of the method are not described suffiently clearly
 in the papers
\begin_inset CommandInset citation
LatexCommand cite
key "semantics_without_access"

\end_inset

 (even though that's on Springer, IEEE), especially the learning process.
 
\lang british
The assumptions above too strict, but at least this is better than just
 modelling individual keywords without their inter-dependences.
\end_layout

\begin_layout Standard

\series bold
Alternatives: CRF and Other Machine Learning Models: 
\end_layout

\begin_layout Standard
Conditional Random Fields (CRFs) which are less restrictive than HMM, have
 been quite popular last years.
 They allow 
\series bold
combining
\series default
 a number of arbitrary features/functions (based on any term of the input
 query and the labels before and after the label that is being decided [of
 even anywhere for more relaxed versions of CRF]) -- all this is not allowed
 in an HMM.
\end_layout

\begin_layout Standard

\color red
I'm still not sure to which extent the manual initialization/bootstrap of
 the model is possible, however at least conversion from HMM into a CRF
 
\series bold
is 
\series default
possible.
 Not yet sure if it's quality of results could outperform the HMM or Exhaustive
 Search.
 Also training could be more complex/expensive for CRF.
 Finally, we may be subject to some limitations of libraries implementing
 HMM or CRF.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
CRFLib implemented in C++ with Python binding seems to spit out only one
 most probable tagging.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question answering and Natural Language Processing
\end_layout

\begin_layout Standard
We are already using Semantic similarity based on Wordnet.
 Some of the other methods could be probably useful, e.g.
 Word Sense Disambiguation (infer exact meaning of a term based on terms
 nearby).
\end_layout

\begin_layout Standard
For processing full-sentence queries, one may consider giving larger weight
 to 
\begin_inset Quotes eld
\end_inset

important
\begin_inset Quotes erd
\end_inset

 words in the query (e.g.
 parse the query and take all Noun Phrases), as well as Question Focus extractio
n (what the question is asking for).
 Rule-based Relation-Extraction could be effective for decoding semantic
 meaning, but they need to be composed manually to be effective, which is
 resource consuming work.

\color magenta
 TODO:Watson also used automatic relation extraction, however I guess that
 needs large and good learning corpus.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
SeCO project
\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods for Feedback and Self-Improvement
\end_layout

\begin_layout Standard
These could include simply boosting the most popular queries (e.g.
 based on structured query logs), like/dislike feedback on particular result
 (a group of similar structured queries) as used SODA
\begin_inset CommandInset citation
LatexCommand cite
key "soda_feedback"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
that is actually almost same as the 
\begin_inset Quotes eld
\end_inset

popularity
\begin_inset Quotes erd
\end_inset

 of an item; this could make more sense if implemented per user role, as
 information need greatly depend on that
\end_layout

\end_inset

), and machine learning methods that could directly incorporate users feedback
 and reason on that.
 Explicit Feedback on entry point generation could be also useful (e.g.
 user choosing that given keyword must correspond to that entity or its
 attribute).
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
That would 
\begin_inset Quotes eld
\end_inset

pollute
\begin_inset Quotes erd
\end_inset

 the UI, but even SODA is using it.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The machine learning that could be initialized with a number of heuristics
 even without query logs and improving with the time, looks quite promising,
 but more effort needed to: a) evaluate if that could at least reach the
 quality of the exhaustive search, and b) how the machine learning and feedback
 mechanism shall be designed.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
In the provided paper on 
\emph on
IR Evaluation
\emph default
 I didn't find anything applicable much for us, except from general ideas
 that often there more interdependences influencing the evaluation than
 being modeled, but that's quite OK for directly evaluating the algorithms.
\end_layout

\begin_layout Standard
Possible evaluation strategies:
\end_layout

\begin_layout Itemize
A predefined set of keyword queries along with their correct interpretation
 (unit tests)
\end_layout

\begin_deeper
\begin_layout Itemize
compare different approaches (e.g.
 pure heuristics, vs.
 HMM without learning)
\end_layout

\end_deeper
\begin_layout Itemize
User's opinion (questionnaire):
\end_layout

\begin_deeper
\begin_layout Itemize
what he prefers: structured queries, keyword queries or natural language
 queries
\end_layout

\begin_layout Itemize
of the three, what gives the best results? does it make the querying easier?
\end_layout

\begin_layout Itemize
user's background
\end_layout

\end_deeper
\begin_layout Itemize
User's performance
\end_layout

\begin_deeper
\begin_layout Itemize
is complex to evaluate.
 if we compared the time required to compose the same structured query:
\end_layout

\begin_deeper
\begin_layout Itemize
there may be individual differences
\end_layout

\begin_layout Itemize
if we asked the same user to compose both structured and keyword queries
 for 
\emph on
the same Information Need
\emph default
, the keyword query would allow user to learn the schema, and that would
 influence his approach to structured queries (but that's still better than
 inverse sequence)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Runtime results (Like/Dislike feedback; Links clicked) - lack of time for
 being useful.
 
\end_layout

\begin_layout Standard
Evaluation of incorporating User feedback into the results ranking is much
 more complex, so I'd be tempted to avoid it (at least for now).

\series bold
\emph on
 
\series default
Probably, additional evaluation results (such as Like/Dislike feedback)
 could be provided during the Defense, that is 3-5 weeks after the Report
 deadline.
\end_layout

\begin_layout Section
Reflections on Innovativeness & Usefulness of this work
\end_layout

\begin_layout Standard
Apart from the fact that it's going to be real world open-source implementation
 (in comparison to closed-source research prototypes) and targeting explicitly
 data services (Keymantic & KEYRY
\begin_inset CommandInset citation
LatexCommand cite
key "keymantic10,bergamaschi2011hidden"

\end_inset

 were mainly designed for DB, but also mentioned web services), there's
 not so much new so far.
 
\end_layout

\begin_layout Standard
On the other hand, usage of Natural Language Processing (NLP) for better
 query interpretation and different Machine Learning (ML) approaches than
 applied before (such as CRF instead of HMM) for incorporating both implicit
 and explicit users feedback, could be something interesting.
 Still I would not be too much excited too early.
 Another problem is that performing a good evaluation of ML approaches could
 be hard as at the moment we do not have learning/evaluation data (gold
 standard).
\end_layout

\begin_layout Standard

\color red
\begin_inset Note Note
status open

\begin_layout Plain Layout

\color red
Review of NLP / Question Answering approaches; hopefully application of
 one or another method
\end_layout

\begin_layout Plain Layout

\color magenta
also there is another project on Question Answering as service integration
 (SeCo) with slitly different goals...
\end_layout

\end_inset


\end_layout

\begin_layout Section
Current Status
\end_layout

\begin_layout Description

\series bold
Done:
\end_layout

\begin_layout Itemize
Literature for: detailed look at HMM approach; NLP and Question Answering;
 some on Feedback and very little on CRFs
\end_layout

\begin_layout Itemize
+/- Structural sketch of Thesis Report
\end_layout

\begin_layout Standard

\series bold
Future tasks:
\end_layout

\begin_layout Itemize
finish automatic extraction of schema and possible values given the meta-data
 and service definitions (includes querying services or gathering info during
 runtime)
\end_layout

\begin_layout Itemize
service statistics to be gathered (avg execution time of each service interface
 + std.
 deviation) - that can be easily computed online with minimal storage
\end_layout

\begin_layout Itemize

\series bold
improve keyword query processor
\end_layout

\begin_deeper
\begin_layout Itemize
incorporate feedback (on results; on keyword classification) : HMM? CRF?
\end_layout

\begin_layout Itemize
allow both structured queries and keywords at the time
\end_layout

\begin_layout Itemize
support for combination of structured input and keywords at the time
\end_layout

\end_deeper
\begin_layout Itemize
evaluation
\end_layout

\begin_layout Standard

\series bold
\emph on
\begin_inset Note Note
status collapsed

\begin_layout Itemize
evaluation!
\end_layout

\begin_layout Plain Layout

\series bold
\emph on
\begin_inset Note Greyedout
status collapsed

\begin_layout Plain Layout

\series bold
\emph on
HMM/CRF Technical details:
\end_layout

\begin_layout Itemize
because vocabulary of possible observations (keywords) is large (including
 their probability matrix 
\emph on
B
\emph default
) not sure if training would work well without a VERY large set of queries.
 How to handle synonyms? We may wish or not, a positive feedback for keyword
 X, to influence it's synonyms also (if we don't 
\end_layout

\begin_layout Itemize
HMM training in NLTK:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
the only facts for additional training could be correct sequences (i.e.
 correct query interpretations).
 what about: a) dislike feedback b) feedback of different reliability (e.g.
 
\emph on
Like
\emph default
 vs 
\emph on
Clicking on one of proposed queries 
\emph default
-- one could never be sure it's 100% correct, and the second is even less
 reliable)
\end_layout

\begin_layout Itemize
no support for ranked list of k-best results (List Viterbi).
 Writting one for 1st order HMM is fairly simple, how about larger order
 (?).
 
\end_layout

\begin_layout Itemize
only the unsupervised training (Baum-Welch) allows passing an HMM model
 to start the training with (this is needed to pass the heuristics); while
 the supervised can take labeled sequences (but no initial model).
 In general Baum-Welch shall be fine.
\end_layout

\begin_layout Itemize
other libraries: a) don't seem to find anything very cool for python b)
 relying on many external libraries means expenses in support
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

The underlying idea is that of defining a conditional probability distribution
 over label sequences given a particular observation sequence, rather than
 a joint distribution over both label and observation sequences.
 The primary advantage of CRFs over hidden Markov models is their conditional
 nature, resulting in the relaxation of the independence assumptions required
 by HMMs in order to ensure tractable inference.
 Additionally, CRFs avoid the label bias problem, a weakness exhibited by
 maximum entropy Markov models (MEMMs) and other conditional Markov models
 based on directed graphical models.
 CRFs outperform both MEMMs and HMMs on a number of real-world tasks in
 many fields, including bioinformatics, computational linguistics and speech
 recognition.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
http://www.inference.phy.cam.ac.uk/hmw26/crf/
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "refs"
options "alpha"

\end_inset


\end_layout

\end_body
\end_document
