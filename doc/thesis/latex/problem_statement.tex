
\section{Problem statement}
\subsection{User Interface and ease of use}

For an IR system with a wide variety of users, it important to provide an easy to use interface with fairly flat learning curve, while not loosing support of fairly complex queries.
%
	Even a simple structured query language plus entity names over the mediated schema  at first may seem hard to learn%
	\footnote{Actually, the names in DAS mediated schema refer to entities fairly consistently named in the real-world.}. Therefore, it shall be useful to guide new users interactively through the process of building the query.
%   
   Supporting non-structured keyword queries is also worth investigation as quite many users reported missing this Google-like search experience.
        
        
\subsection{Supporting complex queries}
Currently DAS can only process queries that could be directly mapped to data providers' APIs, but not the queries where results of one API need to be fed into another APIs 
	%\footnote{this is partially because of performance reasons {\color{red}and the fact that DAS is mainly schema-less (the mediated schema is only in terms of entities, their mappings to APIs and API parameters)}}%
	\footnote{Except a couple of manually hard-coded "views" in a virtual \textit{"combined"} data provider}%
. %In such case, the user  is forced to write some code to submit and process the sub-queries himself. 
Further, to be flexible DAS only keeps a list of 'mediated entities' and how they could be retrieved, but do not enforce any predefined schema (i.e. entity fields/attributes) nor exposes it\footnote{%
	Actually the fields of each API  (an entity could be mapped to a number of these) could be inferred 
	from past queries or by predefined 'bootstrap' queries. Currently the field lists may differ slightly depending on the parameters}
before querying for some particular entity(-ies). 
%
This makes it fairly hard to execute complex queries  without a priori knowledge.
User has to know what queries and how have to be combined (depends on the mediated schema and what APIs are available), and has either to combine the results by hand or to write a program doing that.
%
To be researched:
	\begin{compactitem}
		\item how should these queries be entered by users?

		\item could we use current source descriptions that only define API result type and API parameters as entities of mediated schema to select meaningful and sufficiently efficient composition of services?
	\end{compactitem}
	
%+\item {\color{green} Build series of queries which will answer the given user question, e.g. user asks {\it I want to find files which contains runs taken with magnetic field 4T for given dataset at site X}. This should be decomposed somehow (subject to research) into series of quries, e.g. Find datasets at site X; find files for given dataset list; find runs for found files; filter runs with magnetic field 4T}
       
\subsection{Performance}

DAS is based on \textit{Virtual Integration} where data is left at the sources allowing data to be volatile, e.g. new records gathered or existing ones updated, service's schema changed. Still large amounts of the data are fairly static. 
% %the providers may impose some constraints (e.g. only certain queries allowed).}
As APIs of some data providers' are comparatively slow\footnote{This is partially because of large amounts of data stored at the providers (order of ~1TB per year in total) and as their main goal is supporting data taking from the CMS detector their systems were not optimzed for exploratory queries} it is important to balance between quickly returning items from local cache and getting fresh results. Another approach we are taking is spotting the performance issues at the services (e.g. at DBS some queries requires joining many very large tables, while most of their existing data stays static).
%
Issues to be addressed includes:
\begin{compactitem}
	     				\item At the moment all queries are put into one pool and has to wait until some threads are available. If a couple of heavy queries were submitted earlier, even light queries would have to wait long. Explore more advanced Query prioritization 
	     					(e.g. we could have a query cost model)
						%  based on history or predefined scores per API
	     					
                		\item Currently result items are cached for a fairly short period of time (5min-1h) and then completely discarded, however many entities are not changing that often - Explore more intelligent caching
	                	  
	                		% TODO: \item query rewriting then used with grep=smf then the same item is available as selection key. check how many instances.
	                		

					\item Since DAS does aggregation across multiple data providers, given their current APIs (with no \textit{ordering} and \textit{paging} of results), DAS has to fetch ALL records matching the query instead of only the first page\footnote{Although it would be possible to show partial results for data coming from a single data-source (still loading in background, or the providers' APIs would have to be modified)}.		
					\item Efficient distributed search: as filtering criteria may reside on two or more autonomous sources, given the current APIs much more items than in the final result may need to be fetched from each source. Items that do not meet all selection criteria could be filtered out only afterwards in DAS. To improve the performance, data provider's APIs for the most used queries may need to be redesigned.
                        % \item {\color{red}e.g. displaying intermediary results of aggregation while still calculating with statistical bounds -- however statistical bounds are much meaningful only if items are well snuffled/randomized -- people could stop the query if they see a non-sense}
	              %\item {\color{red}scale testing - if we are storing lots of historical info. MongoDB is not so performant if DB cant fit in memory. One of well known solutions is installing SSD.                        }
\end{compactitem}

\subsection{\color{gray}Generic connector accessing relational databases}

{\color{gray}A generic connector accessing relational databases with minimal human effort could be useful for integrating proprietary systems that were not yet integrated when there's no resources to build the APIs. A possible use-case could be the \textit{Prep} database, but this still has to be discussed. 
%
%Depending on the needs it could be either of exploratory or API/Query Forms based approach, where fake APIs are defined in terms of SQL queries (or even better in a simplified form that would generate SQL by taking schema into account).
There was a project\footnote{\url{https://github.com/vkuznet/PyQueryBuilder}}   at CERN which tried to generalize DBS-Query Language for any database that could be used as DB side mediator.
}
% \color{green} Indeed, an dthat was the topic of research for another DAS student, Liang Dong. The idea was to generalize DBS-QL to any DB back-end and provide common package which will do a job. The work is done over here . Feel free to check it out and we can discuss it more.}
