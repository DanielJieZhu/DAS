\documentclass[a4paper,11pt,draft]{article}
\special{papersize=210mm,297mm}

% twocolumn
%\usepackage{fullpage} 
\usepackage[cm]{fullpage}
%\usepackage[margin=0.5in]{geometry}
%\usepackage{times}
\usepackage{color}
%\usepackage[small,compact]{titlesec}

\bibliographystyle{alpha} 
 
% usually first paragraph is NOT indented, so this is commented out: 
% \usepackage{indentfirst} 
 
\begin{document}
 
% Article top matter
\title{Master thesis problem statement: Keyword search over heterogeneous sources} %\LaTeX is a macro for printing the Latex logo
\author{Vidmantas Zemleris}  %\texttt formats the text to a typewriter style font
\date{\today}  %\today is replaced with the current date
 
%\maketitle

%
 \centerline{\Large \bf Searching heterogeneous data-sources: Master thesis problem statement} %% Paper title
 \medskip
 
 \centerline{Vidmantas Zemleris, \today}  %% Author name(s)
 \medskip
 


 
\section{Introduction}
At large scientific collaborations like the CMS Experiment at CERN's LHC that includes more than 3000 collaborators data usually resides on a fair number of autonomous and heterogeneous proprietary systems each serving it's own purpose
\footnote{For instance, at CERN,  due to many reasons (e.g. research and need of freedom, politics of institutes involved) software projects usually evolve in independent fashion resulting in fair number of proprietary systems\cite{Koch00CERN}. Further high turnover makes it harder extending these systems}. As data stored on one system may be related to data residing on other systems%
	\footnote{For example, datasets containing physics events are registered at DBS, while the physical location of files is tracked by Phedex which also takes care of their transfers within the worldwide grid storage}%
, users are in need of a centralized and easy-to-use solution for locating and combining data from all these multiple services.

Using a highly structured language like SQL is problematic because users need to know not only the language but also where to find the information and also lots of technical details like schema. A data integration system based on simple structured queries is already in place. Still {\color{red}various improvements including support} for less restricted keyword queries and improvements to system's usability and performance still have to be researched.

% {\color{red}A remarkable opportunity we have is possibility to test ideas on a sample of ~3000 people working at CMS.}


\section{Case study: the CMS Experiment at CERN}
Users' information need may vary greatly depending on their role, however most of the time they are interested in locating a \textit{full set} of entities matching some selection criteria, e.g.:
%\footnote{in contrary to exploratory approach where  a subset of best matches is enough}
   \begin{itemize}
  		\item find all \textit{files} from \textit{dataset(s)} matching wild-card query each containing some of the 'interesting' \textit{runs} from a list provided (Release validation teams)
         \item find (all) \textit{datasets} related some specific physics phenomena\footnote{In case of dataset this data is present in filename or run} together with conditions describing how this data was recorded by detector or simulated which are present in separate autonomous system than the datasets (Physicists)
         % TODO:is this a good example? conditions is a separate query now...
         \item find all \textit{datasets} matching some pattern stored at a given \textit{site} (filtering on entities stored on separate services)
   \end{itemize}                		
% In such dynamic environment it is preferred to choose local-as-view approach for information integration instead of creating a global schemata\cite{Koch00CERN}.

For more use-cases of data retrieval at CMS Experiment see \cite{CMS_data08}.


%Following a similar approach a 
\subsection*{The Data Aggregation System}

The Data Aggregation System (DAS)\cite{Kuznetsov2010, Kuznetsov2011} was created which allows integrated access to a number of proprietary data-sources by processing user's queries on demand - it queries the data-sources, merges the results, and caches them for subsequent use. DAS uses \textit{Boolean retrieval model} as users are often interested in retrieving ALL the items matching their query.

Currently the queries specify what entity the user is interested in (dataset, file, etc) and provide selection criteria (attribute=value, name BETWEEN [v1, v2]) operators. The combined query results could be later 'piped' for further filtering and aggregation (min, avg, etc), e.g.:

{\footnotesize 
\begin{verbatim}
dataset=*RelVal* | grep dataset.nevents >1000 | avg(dataset.size), median(dataset.size)
\end{verbatim}
}

The query above would return average and median datasets sizes  of ones containing  \textit{RelVal} in their name having more than 1000 events.

Queries could be run either from web browser or through  command line interface where the results could  be fed into another application (e.g. program doing physics analysis or automatic software release validation).


% \textbf{\color{red}Examples of more general uses: Digital libraries, Stock markets, [ Weather, Flight costs]}

\section{Problem statement}

DAS is based on \textit{Virtual Integration} where data is left at the sources allowing data to be volatile (e.g. new records gathered or existing records updated; service descriptions may be changed; new services added), while some subsets of the data could be fairly static (e.g. datasets stored at DBS do not change often). 
Further as the total data that is being queried could be fairly large (with growth of order of 1TB per year) and the data-services may have certain constraints (e.g. only certain queries allowed) as their main goal is supporting production (data taking from the CMS detector).

Therefore, it is important to balance between returning locally cached query results quickly   and between issuing queries to slow data-services to get up to date results.
It is also not less important to provide users with easy to use interface having a fairly flat learning curve, while at the same time not loosing the functionality of fairly complex queries. % expressive power.
\textbf{\color{red}Another issue is about handling distributed queries, where either selection criteria or selection values could come from distributed sources -- the biggest problem could be in the APIs they [DO NOT] provide}.

%TODO: Further it has to comply with dataset constraints

% \textbf{\color{red}DO WE also have proprietary big databases that could be useful to be searched, but the standard integration work would be to heavy (or sub-parts of existing systems that are not covered by APIs, and could be accessible through DBs directly -- these potentially more expensive queries could processed with lower priority)}



\subsection{Ease of use: Query Language over the mediated schema}
	Even a very simple structured query language that also contain entity names over the mediated schema may seem {\color{red}hard} to learn, especially in the beginning.
   On the other hand, at CERN the names in  the mediated schema are referring to real-world entities that {\color{red}are fairly consistently named} (even though there may exist slight differences in their naming on different data-sources). 
   So some sort of guiding helping user to build the query shall be useful. 
   
   Supporting non-structured keyword queries is also worth investigation as many users reporting they are missing this Google-like search experience. {\color{red}Because DAS uses Boolean IR we could only rank specific structured queries.}
        
	Further a minimum set of search predicates is imposed by APIs (mainly because of performance reasons) and user has to be at least informed what is he expected to provide.
                
                
Also to consider: input ambiguity and typos, some queries are more common than others.

\subsection{Performance}

\begin{itemize}
	     					\item  Query prioritization: {\color{red}maybe we could have prioritization in general, the heavier your query is the more you have to wait in favour of  the light queries}: 
	     					(we could have some sort of query cost evaluation based on history or manually predefined scores per API)
	     					
                		\item Explore more intelligent caching
	                	  
	                		% TODO: \item query rewriting then used with grep=smf then the same item is available as selection key. check how many instances.
	                		

					\item Could displaying Partial Results improve performance?
					\begin{itemize}
					     \item given current APIs to do aggregation over data sources, DAS has to fetch ALL records matching the query instead of only the first page.
%                        \item ??? do we have cases then multiple APIs of the same service could be called for the same data, can we choose just one???
                        \item {\color{red}e.g. intermediary aggregation results while still calculating}
                        
					\end{itemize}

                        
	                	 \item scale testing - if we are storing lots of historical info. MongoDB is not so performant if DB cant fit in memory. One of well known solutions is installing SSD.                        
\end{itemize}

\subsection{Handling distributed search efficiently}


\pagebreak
\include{proposed_solutions}


\pagebreak
\include{literature_review}



\section{Work status}
TODO

\begin{verbatim}
- obtained DB copy of biggest data provider DBS (currently 80 GB + 200 GB indexes)
- preliminary literature review (to be continued deeper + waiting for book arrival: Principles of Data Integration)

--- some analysis of DAS logs


Upcomming Work items:
- couple of fairly simple prototypes of UI/access patters for simpler DAS querying
- check on performance improvements after creation of materialized view(s)
- interview (more) DAS users
\end{verbatim}

% ---  Bibliography
\bibliographystyle{unsrt}
\addcontentsline{toc}{chapter}{Bibliography}

%\nocite{*}
\thispagestyle{empty}
\bibliography{refs}

\pagebreak
\appendix
\section{DAS Query logs}

non parsible queries: 98923

\subsection*{Most common query patterns (Feb 2012-June 2012)}
\begin{verbatim}
total valid queries: 569408

50.16% (285605) :	dataset dataset.name=?
13.54% (77071)  :	site dataset.name=?
8.96%  (51035) 	:	file dataset.name=?
5.88%  (33504) 	:	run dataset.name=?
2.59%  (14739) 	:	release dataset.name=?
2.11%  (12036) 	:	config dataset.name=?
1.65%  (9420) 	:	dataset run.run_number=?
1.34%  (7642) 	:	block dataset.name=?
1.24%  (7084) 	:	dataset site.name=?
1.15%  (6562) 	:	dataset dataset.name=? release.name=?
1.10%  (6287) 	:	parent dataset.name=?
0.96%  (5488) 	:	file file.name=?
0.92%  (5245) 	:	dataset dataset.name=? status.name=?
0.77%  (4363) 	:	dataset release.name=?
0.75%  (4257) 	:	file dataset.name=? run.run_number=?
0.68%  (3874) 	:	run run.run_number=?
0.53%  (2994) 	:	site site.name=?
0.45%  (2576) 	:	site file.name=?
0.45%  (2556) 	:	dataset file.name=?
0.43%  (2438) 	:	lumi file.name=?
0.40%  (2282) 	:	dataset dataset.name=? site.name=?
0.35%  (1999) 	:	file block.name=?
0.35%  (1970) 	:	dataset dataset.name=? run.run_number=?
0.29%  (1640) 	:	group dataset.name=?
0.29%  (1631) 	:	lumi run.run_number=?
0.25%  (1402) 	:	child dataset.name=?
0.22%  (1268) 	:	run file.name=?
0.21%  (1204) 	:	block block.name=?
0.19%  (1088) 	:	release release.name=?
0.16%  (936) 	:	site block.name=?
0.12%  (703) 	:	file dataset.name=? lumi.number=? run.run_number=?
0.10%  (594) 	:	parent file.name=?
0.08%  (455) 	:	dataset block.name=?
0.06%  (365) 	:	dataset dataset.name=? datatype.name=? release.name=?
0.06%  (360) 	:	file dataset.name=? site.name=?

Interesting non structured queries:
T!_CERN
*herwig*/AODSIM

\end{verbatim}
% py dataset=/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball/Summer12_DR53X-PU_S10_START53_V7A-v1/AODSIM


\end{document}
