\documentclass[a4paper,11pt]{article}
% ,draft
\special{papersize=210mm,297mm}

% twocolumn
%\usepackage{fullpage} 
\usepackage[cm]{fullpage}

%\addtolength{\topmargin}{-10mm}
%\addtolength{\topskip}{-10mm}

%\usepackage[margin=0.5in]{geometry}
%\usepackage{sans}
%\usepackage{lmodern}
\usepackage{paralist}
\usepackage{color}

% enable Hyperlinks
\usepackage{hyperref}
% fancy verbatim
\usepackage{fancyvrb}
\usepackage{appendix}
\usepackage[usenames,dvipsnames]{xcolor}


%\usepackage{alltt}

%\usepackage[small,compact]{titlesec}

\bibliographystyle{alpha} 
 
% usually first paragraph is NOT indented, so this is commented out: 
% \usepackage{indentfirst} 
 
\begin{document}
%\addtolength{\bottommargin}{-10mm}
\addtolength{\textheight}{10mm}

 
% Article top matter
\title{Master thesis problem statement: Keyword search over heterogeneous sources} %\LaTeX is a macro for printing the Latex logo
\author{Vidmantas Zemleris}  %\texttt formats the text to a typewriter style font
\date{\today}  %\today is replaced with the current date
 
%\maketitle

%
 \centerline{\Large \bf Searching heterogeneous data-sources: Master thesis problem statement} %% Paper title
 \medskip
 
 \centerline{Vidmantas Zemleris, \today}  %% Author name(s)
 \medskip
 


 
\section{Introduction}
At large scientific collaborations like the CMS Experiment at CERN's LHC that includes more than 3000 collaborators data usually resides on a fair number of autonomous and heterogeneous proprietary systems each serving it's own purpose
\footnote{For instance, at CERN,  due to many reasons (e.g. research-orientedness and need of freedom, politics of institutes involved) software projects usually evolve in independent fashion resulting in fair number of proprietary systems\cite{Koch00CERN}. Further high turnover makes it harder to extend these systems}. As data stored on one system may be related to data residing on others%
	\footnote{For example, datasets containing physics events are registered at DBS, while the physical location of files is tracked by Phedex which also takes care of their transfers within the grid storage worldwide}%
, users are in need of a centralized and easy-to-use solution for locating and combining data from all these multiple services.

Using a highly structured language like SQL is problematic because users need to know not only the language but also where to find the information and also lots of technical details like schema. An enterprise information integration system based on simple structured queries is already in place. Various improvements  still have to be researched: adding support for more complex queries and for less restricted keyword search, improvements to system's usability and performance.

% {\color{red}A remarkable opportunity we have is possibility to test ideas on a sample of ~3000 people working at CMS.}


\section{Case study: the CMS Experiment at CERN}
Users' information need may vary greatly depending on their role, however most of the time they are interested in locating a \textit{full set} of entities matching some selection criteria, e.g.:
%\footnote{in contrary to exploratory approach where  a subset of best matches is enough}
   \begin{itemize}
  		\item find all \textit{files} from \textit{dataset(s)} matching wild-card query each containing some of the 'interesting' \textit{runs} from a list provided (Release validation teams)
         \item find all \textit{datasets} matching some pattern stored at a given \textit{site} 
         	  \begin{small}(filtering attributes from separate services)\end{small}
         \item find (all) \textit{datasets} related to some specific physics phenomena\footnote{In case of dataset, this data is present in the \textit{filename} or in the \textit{run} entity} together with conditions describing how this data was recorded by detector or simulated which are present in another autonomous system than the datasets (Physicists)
         % TODO:is this a good example? conditions is a separate query now...
   \end{itemize}                		
% In such dynamic environment it is preferred to choose local-as-view approach for information integration instead of creating a global schemata\cite{Koch00CERN}.

For more use-cases of data retrieval at CMS Experiment see \cite{CMS_data08}.

%Following a similar approach a 
\subsection*{The Data Aggregation System}

The \textit{CMS Data Aggregation System (DAS)}\cite{Kuznetsov2010, Kuznetsov2011} was created which allows integrated access to a number of proprietary data-sources by processing user's queries on demand - it determines  data-sources are able to answer\footnote{This is done by a mapping between flat mediated schema entities ('DAS keys') into web-service methods and their arguments. Then system queries all services that could provide a result of expected type with given parameters}, queries them, merges the results and caches them for subsequent use. DAS uses \textit{Boolean retrieval model} as users are often interested in retrieving ALL the items matching their query.

Currently the queries specify what entity the user is interested in (dataset, file, etc) and provide selection criteria (attribute=value, name BETWEEN [v1, v2]). The combined query results could be later 'piped' for further filtering, sorting or aggregation (min, max, avg, sum, count, median), e.g.:

{\small 
\begin{Verbatim}[commandchars=\\\{\}]
\footnotesize# returns average and median dataset sizes of ones containing \textit{RelVal} in their name having more than 1000 events.
dataset=*RelVal* | grep dataset.nevents >1000 | avg(dataset.size), median(dataset.size)
\end{Verbatim}
}

Queries are executed either from web browser or through a command line interface where the results could be fed into another application (e.g. program doing physics analysis or automatic software release validation).
% \textbf{\color{red}Examples of more general uses: Digital libraries, Stock markets, [ Weather, Flight costs]}
\input{problem_statement}

\include{literature_review}

\input{proposed_solutions}



\newpage
\section{Work status}
{\begin{footnotesize}
\begin{verbatim}
- set-up of development environment
- preliminary literature review (to be continued deeper)
- initial analysis of DAS logs
- obtained DB copy of biggest data provider DBS (currently 80 GB + 200 GB indexes)
- some work done on solving wildcard query restrictions

Upcomming Work items:
- benchmark data provider's performance (per API based on historical queries)
- couple of fairly simple prototypes of UI/access patters for simpler DAS querying
- check performance improvements after creation of materialized view(s)
- look into possible implementations for combined queries

\end{verbatim}
\end{footnotesize}
}
% ---  Bibliography
\bibliographystyle{unsrt}
\addcontentsline{toc}{chapter}{Bibliography}

%\nocite{*}
\thispagestyle{empty}
\begin{small}
\bibliography{refs}
\end{small}

\pagebreak
\input{milestones}

\pagebreak
\input{das_logs}

{\color{gray}
\subsubsection*{Not so relevant from Literature Review: Deep web search at Google}
%[multiple papers from Google] discusses various ways for implementing data integration in terms of large-scale search engine (Google): Virtual integration vs. Surfacing. They also present ways for integrating systems without human intervention through use of statistical 'mediator'.

There are two approaches to web scale search over deep-web (here Google mostly cares about web forms): 
%\marginpar{\scriptsize\color{red}TODO: how does google use PayGo?} 
%\textit{Runtime query reformulation} - 'leaves data at the sources and routes queries to appropriate services'\cite[p. 1]{webscale_paygo} 

\textit{Deep-web surfacing} - surface deep-web (e.g. web forms) adding their results into the standard search index  easily allowing to using existing IR technology that scales well. There exist algorithms which allow to iteratively choose input parameters to the forms to surface a considerable part of the 'hidden' data without large overhead%
	\footnote{i.e. if choosing parameters in not smart way, a web form with just a couple of free inputs (or even dropdowns that's easier case), could yield as many results as a cross product of all input combinations.}.

\textit{Pay-as-you-go approach}\cite{webscale_paygo} - With this approach, 'a system starts with
very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary';
% TODO: cite{bootstraping}
there is NO single mediated schema over which users pose queries: queries are routed to the relevant sources with help statistical methods that are used to model uncertainty at all levels: queries, mappings and underlying data. 
}

\end{document}
