\section{Proposed solutions}
\subsection{Ease of use}

\begin{itemize}
\item Interactive guidance on how to compose queries to ease the learning (could also include some examples of popular queries):
\begin{enumerate}
\item What entity are you searching for?
\item How could you identify it (i.e. what do you know)?
\item What do you want to do with these results (after seeing the results)? Possible options: list items; select only specific columns; do aggregation; use in command line/another program
\end{enumerate}


\item Given a keyword query, suggest the best matching structured queries (as a ranked list)%
 	\footnote{As DAS uses Boolean IR model we could only rank specific structured queries, but not the results itself.}:
	\begin{itemize}
	\item map keywords to entity names and selection attributes (quite resembling 'Query forms' and 'Keyword cleaning' approaches)
	\item build an inverted index of attribute values with some good full-text search toolkit\footnote{e.g. Xapian (http://xapian.org/) implemented in C++ which that is known for its flexibility, or Sphinx (http://sphinxsearch.com/)}. If no match found (e.g. new entry not yet in our cache) pattern matching could be used to make a best guess.
	\item ranking could be based on: how closely keywords are matching (some or all) required API's input parameters, popularity of certain queries and users feedback
	
	\item {\color{red} evaluate different user feedback approaches (e.g. HMM)
	\item multi-wildcard queries (e.g. dataset=*Zmm*special*RECO*) are common at CMS that will need special attention\footnote{Matching to structured query will be harder and guessing even worse. Also for performance reasons some web-services will introduce limitations on the format of wilcard queries, while DAS wants still to be flexible at least for datasets}.}
	
\item making use of direct access to database {\color{red}(if available; in theory)}
\begin{itemize}
\item one could do incremental indexing of a target database  \footnote{E.g. an inverted index of values in Oracle/MySQL DB tables can be built to keep the list of possible terms. For instance, Sphinx full text search engine can access DBs directly and also supports incremental indexing but it needs a bit of manual configuration {http://sphinxsearch.com/docs/current.html\#delta-updates} }, then try to map database table columns into API parameters (manually or even automatically by applying some of Data Integration approaches)
\item then this index + mapping could be used to improve the keyword search (better mapping from keywords to API inputs) or even structured search (e.g. query cleaning/term auto-completion)
\item problem: the owners of services may not want to provide direct access to DB because of no trust, security or performance issues
\item may need to exclude very large tables
\end{itemize}
	\end{itemize}
\item (?) JavaScript-based query interpreter to ease query writing: could suggest search attribute and entity attribute names. also taking into account some possible ambiguous namings (auto completion)
	
	
\end{itemize}


\subsection{Performance}


%%%%%  -------------------------------------------

\subsubsection*{More intelligent caching}
Some of the data entities instances changing very rarely, for example in DBS system old datasets would never change, while new ones are constantly added (still some of their attributes may change, in this example validity). 
%
Cached copy may be shown by default while up to date results could be retrieved on user's request. An automatic change rate prediction could be useful to efficiently balance between caching and retrieving results.

On the other hand some result items may not make sense to be cached, as there may be too many of valid input parameter combinations (e.g. now there are 900M of \textit{(run, file)} combinations).

Also consider:
                			\begin{itemize}
                			%\item given an entity received from provider, determining if it is useful to cache for long-term
%	                			\item \textbf{\color{red}At what level are we caching now? query or individual query results?}
                			%\item could cache even \textit{expired} data, but warning user
                			\item could also have different validity dates for certain fields. if no volatile fields are not explicitly requested, even a very old cache could be used. 
                			\item Can we automatically figure which fields are static and which are changing?
                			\item pre-fetching common (sub-)queries: determining manually and/or automatically
                			\end{itemize}





%%%%%  -------------------------------------------
\subsubsection*{Continuous view maintenance at Data providers with large DBs}
In the cases when new records are coming not the existing ones are not changing much, continuous view maintenance that computes only differences from earlier results could be a good solution to improve the performance. 


Use either \textit{materialized refresh fast views with query rewriting} (Oracle; completely transparent for proprietary apps)\cite{Oracle11}
 or some other continuous view maintenance tool (e.g. DBToaster\footnote{http://www.dbtoaster.org that is being developed at EPFL}) to improve performance of heavy queries containing joins and/or aggregations.

For example currently in DBS (80GB + 280GB indexes) to process a query 'find files where run in [r1, r2, r3] and dataset=X' one needs to join all the tables below.

\begin{verbatim}
Dataset (164K rows) -> Block (2M) -> Files (31M) -> FileRunLumi (902M) <- Runs (65K)
\end{verbatim}

However having a materialized view of all these tables joined together would allow answering such queries much quicker. Given low change rates (in comparison to data already present), maintaining the view should be also comparatively cheap with only expense of just couple of times of space.


%%%%%  -------------------------------------------

\subsection{Integrating distributed information efficiently}
Bloom-join (which could be quite transparent and implemented even on DB side [pure SQL is possible for MySQL, to check for Oracle]- take a query and bit-vector as parameter) , lazy pagination (and order required for aggregation) - this is not yet supported by any of the data-service APIs

%\newline
% integration at DAS level. (at source DB could be more performant a little)

