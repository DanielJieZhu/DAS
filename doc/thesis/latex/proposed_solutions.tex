\section{Proposed solutions}
\subsection{Ease of use}

\begin{itemize}
\item Interactive guidance on how to compose queries to ease the learning (could also include some examples of popular queries):
\begin{enumerate}
\item What entity are you searching for?
\item How could you identify it (i.e. what do you know)?
\item What do you want to do with these results (after seeing the results)? Possible options: list items; select only specific columns; do aggregation; use in command line/another program
\end{enumerate}


\item Given a keyword query, suggest the best matching structured queries (as a ranked list)%
 	\footnote{As DAS uses Boolean IR model we could only rank specific structured queries, but not the results itself.}:
	\begin{itemize}
	\item the algorithm will generate mappings between each keyword to either an entity name, selection attribute name or its value (resembling 'Query forms' and 'Keyword cleaning' approaches)
	\item ranking could be based on: how closely keywords are matching (some or all) required API's input parameters, popularity of certain queries and users feedback	
	\item build an inverted index of attribute values with some good full-text search toolkit%
				\footnote{e.g. Xapian (http://xapian.org/) implemented in C++ which that is known for its flexibility, or Sphinx (http://sphinxsearch.com/)}. 
	If no match found (e.g. new entry not yet in our cache) pattern matching could be used to make a best guess
	\item the system could improve its results from users' feedback (e.g. by HMM, will need to decide)
	\item multi-wildcard queries (e.g. dataset=*Zmm*special*RECO*) are common at CMS that will need special attention\footnote{Matching to structured query will be harder and guessing even worse. Also for performance reasons some web-services will introduce limitations on the format of wilcard queries, while DAS wants still to be flexible at least for datasets}.
	
\item knowing all possible APIs parameter values would be beneficial ({\color{red}but not yet sure if available}%
		\footnote{problem: the owners of services may not want to provide direct access to DB because of no trust, security or performance issues. Otherwise, specific APIs could be developed to return possible parameter values but this is quite an overhead}) and could be used to improve the keyword search (better mapping from keywords to API inputs) or even the structured search (e.g. query cleaning/term auto-completion)
\begin{itemize}
\item having direct access to DB, one could incrementally index target database\footnote{%
			E.g. an inverted index of values in Oracle/MySQL DB tables can be built to keep the list of possible terms. For instance, Sphinx full text search engine can access DBs directly and also supports incremental indexing but it needs a bit of manual configuration {http://sphinxsearch.com/docs/current.html\#delta-updates} }%
	tables mapping it's columns into API parameters (manually or automatically). 

%\item may need to exclude very large tables
\end{itemize}
	\end{itemize}
\item (?) JavaScript-based query interpreter to ease query writing: could suggest search attribute and entity attribute names. also taking into account some possible ambiguous namings (auto completion)
\end{itemize}


\subsection{Supporting complex queries}
As most entities in the mediated schema (27 of them) are already mapped to multiple services (see the mapping here: \url{https://cmsweb.cern.ch/das/services}) these taken as a graph could be used to build (some of) the more complex queries.

\subsection{Performance}


%%%%%  -------------------------------------------

\subsubsection*{More intelligent caching}
Some of the data entities instances changes very rarely. For example in DBS system old datasets would never change, while new entities are constantly added (still some of their attributes may change, e.g.  validity of a dataset). 
%
Cached copy may be shown by default while up to date results could be retrieved on user's request. An automatic change rate prediction could be useful to efficiently balance between caching and retrieving results.
%
On the other hand some result items may not make sense to be cached, as there may be too many of valid input parameter combinations (e.g. now there are 900M of \textit{(run, file)} combinations).

Also consider that:
                			\begin{itemize}
                			%\item given an entity received from provider, determining if it is useful to cache for long-term
%	                			\item \textbf{\color{red}At what level are we caching now? query or individual query results?}
                			%\item could cache even \textit{expired} data, but warning user
                			\item we could also have different validity dates for certain fields. if no volatile fields are not explicitly requested, even a very old cache could be used. 
                			% \item Can we automatically figure which fields are static and which are changing?    			 {\color{green} Yes, if we enfore APIs to record modification timestamp} {\color{red} I also meant some automatic algorithm, however that would be more complex and less reliable}
                			\item pre-fetching common queries: determining manually and/or automatically
                			\end{itemize}





%%%%%  -------------------------------------------
\subsubsection*{Data providers performance issues}
In the cases when new records are coming but the existing ones are not changing much, continuous view maintenance that computes only differences from earlier results could be a good solution to improve the performance especially for often used queries. 

Use either \textit{materialized refresh fast views with query rewriting} (Oracle; completely transparent for proprietary apps)\cite{Oracle11}
 or some other continuous view maintenance tool (e.g. DBToaster\footnote{http://www.dbtoaster.org that is being developed at EPFL}) to improve performance of heavy queries containing joins and/or aggregations.

This is exactly the case for the most popular expensive query (see \hyperref[run_dataset_heavy]{query \#\ref{run_dataset_heavy} in \ref{appendix_das_cli_logs} DAS log analysis}) over DBS system (80GB + 280GB indexes): 'find files where run in [r1, r2, r3] and dataset=X' that needs to join all the biggest tables in that database.

\begin{verbatim}
Dataset (164K rows) -> Block (2M) -> Files (31M) -> FileRunLumi (902M) <- Runs (65K)
\end{verbatim}

Having a materialized view with all these tables joined together would allow answering such queries much quicker. Given low change rates (in comparison to data already present), maintaining the view should be also comparatively cheap with only expense of just couple of times of space.


%%%%%  -------------------------------------------

\subsubsection*{Integrating distributed information efficiently}
Bloom-join (which could be quite transparent and implemented even on DB side [pure SQL is possible for MySQL, to check for Oracle]- take a query and bit-vector as parameter) , lazy pagination (and order required for aggregation) - this is not yet supported by any of the data-service APIs

%\newline
% integration at DAS level. (at source DB could be more performant a little)

